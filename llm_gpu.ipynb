{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Print the Python version\n",
        "print(\"Python version\")\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1lgJ9og8r25",
        "outputId": "47145e07-07e5-41ca-dae7-56adba248bd3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version\n",
            "3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD8axkr_9541",
        "outputId": "c6f40a55-a6ee-43e9-ea06-ac5913c7eedf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec  1 11:40:36 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub>=0.17.1\n",
        "!pip -q install --upgrade fschat accelerate autoawq vllm\n",
        "!pip install torch==2.1.0+cu121 torchvision==0.16.0+cu121 torchaudio==2.1.0 torchtext==0.16.0+cpu torchdata==0.7.0 --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy-4v6Qg-pGP",
        "outputId": "fbaf343d-ce14-4c56-c4b8-e47944d3619b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch==2.1.0+cu121 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.16.0+cu121 in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchtext==0.16.0+cpu in /usr/local/lib/python3.10/dist-packages (0.16.0+cpu)\n",
            "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0+cu121) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0+cu121) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0+cu121) (9.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0+cpu) (4.66.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0+cu121) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0+cu121) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0+cu121) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0+cu121) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0+cu121) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "huggingface_hub.login(token='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')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8Cp6XZDcVlQ",
        "outputId": "e21efdf2-45cf-4978-a07e-245bbb884767"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-AWQ --local-dir ./Mistral-7B-Instruct-v0.1-AWQ --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWjnGI-WcVoG",
        "outputId": "25e3d40d-665b-478d-f721-1a58f25f33c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching 10 files:   0% 0/10 [00:00<?, ?it/s]downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/model.safetensors to /root/.cache/huggingface/hub/tmpw330vv3o\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/config.json to /root/.cache/huggingface/hub/tmpy0h_viju\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/quant_config.json to /root/.cache/huggingface/hub/tmp3s0p6n5m\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/special_tokens_map.json to /root/.cache/huggingface/hub/tmp1xw76xnm\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/generation_config.json to /root/.cache/huggingface/hub/tmpw9ekd0aq\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/README.md to /root/.cache/huggingface/hub/tmp8838fxyz\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/tokenizer.json to /root/.cache/huggingface/hub/tmp6qy1njtl\n",
            "\n",
            "Downloading config.json: 100% 757/757 [00:00<00:00, 3.81MB/s]\n",
            "Storing https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/config.json in local_dir at ./Mistral-7B-Instruct-v0.1-AWQ/config.json (not cached).\n",
            "\n",
            "Downloading quant_config.json: 100% 90.0/90.0 [00:00<00:00, 604kB/s]\n",
            "Storing https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/quant_config.json in local_dir at ./Mistral-7B-Instruct-v0.1-AWQ/quant_config.json (not cached).\n",
            "\n",
            "Downloading (…)cial_tokens_map.json: 100% 72.0/72.0 [00:00<00:00, 466kB/s]\n",
            "Storing https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/special_tokens_map.json in local_dir at ./Mistral-7B-Instruct-v0.1-AWQ/special_tokens_map.json (not cached).\n",
            "\n",
            "Downloading generation_config.json: 100% 116/116 [00:00<00:00, 721kB/s]\n",
            "Storing https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/generation_config.json in local_dir at ./Mistral-7B-Instruct-v0.1-AWQ/generation_config.json (not cached).\n",
            "\n",
            "Downloading README.md: 100% 11.0k/11.0k [00:00<00:00, 45.9MB/s]\n",
            "Storing https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/README.md in local_dir at ./Mistral-7B-Instruct-v0.1-AWQ/README.md (not cached).\n",
            "\n",
            "Downloading tokenizer.json:   0% 0.00/1.80M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   0% 0.00/4.15G [00:00<?, ?B/s]\u001b[A\u001b[Adownloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/tokenizer.model to /root/.cache/huggingface/hub/tmp_lgw1egh\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/.gitattributes to /root/.cache/huggingface/hub/tmpuw_969k8\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/tokenizer_config.json to /root/.cache/huggingface/hub/tmpf8vm2aun\n",
            "Downloading tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 26.6MB/s]\n",
            "Storing https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/tokenizer.json in local_dir at ./Mistral-7B-Instruct-v0.1-AWQ/tokenizer.json (not cached).\n",
            "\n",
            "Downloading .gitattributes: 100% 1.52k/1.52k [00:00<00:00, 7.22MB/s]\n",
            "Storing https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/.gitattributes in local_dir at ./Mistral-7B-Instruct-v0.1-AWQ/.gitattributes (not cached).\n",
            "Fetching 10 files:  10% 1/10 [00:01<00:13,  1.51s/it]\n",
            "Downloading tokenizer_config.json: 100% 962/962 [00:00<00:00, 4.79MB/s]\n",
            "Storing https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/tokenizer_config.json in local_dir at ./Mistral-7B-Instruct-v0.1-AWQ/tokenizer_config.json (not cached).\n",
            "\n",
            "\n",
            "Downloading model.safetensors:   0% 10.5M/4.15G [00:00<01:34, 43.8MB/s]\u001b[A\u001b[A\n",
            "Downloading tokenizer.model: 100% 493k/493k [00:00<00:00, 13.6MB/s]\n",
            "Storing https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/tokenizer.model in local_dir at ./Mistral-7B-Instruct-v0.1-AWQ/tokenizer.model (not cached).\n",
            "\n",
            "\n",
            "Downloading model.safetensors:   1% 21.0M/4.15G [00:00<01:18, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   1% 31.5M/4.15G [00:00<01:01, 66.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   1% 41.9M/4.15G [00:00<00:53, 77.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   2% 62.9M/4.15G [00:00<00:41, 98.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   2% 83.9M/4.15G [00:00<00:33, 123MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   3% 105M/4.15G [00:00<00:27, 145MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   3% 126M/4.15G [00:01<00:27, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   4% 147M/4.15G [00:01<00:28, 141MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   4% 168M/4.15G [00:01<00:26, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   5% 189M/4.15G [00:01<00:27, 143MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   5% 210M/4.15G [00:01<00:26, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   6% 231M/4.15G [00:01<00:25, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   6% 252M/4.15G [00:01<00:25, 154MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   7% 273M/4.15G [00:02<00:26, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   7% 294M/4.15G [00:02<00:28, 138MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   8% 315M/4.15G [00:02<00:31, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   8% 336M/4.15G [00:02<00:31, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   9% 357M/4.15G [00:02<00:29, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   9% 377M/4.15G [00:02<00:28, 134MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  10% 398M/4.15G [00:03<00:27, 137MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  10% 419M/4.15G [00:03<00:28, 132MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  11% 440M/4.15G [00:03<00:25, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  11% 461M/4.15G [00:03<00:26, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  12% 482M/4.15G [00:03<00:24, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  12% 503M/4.15G [00:03<00:23, 153MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  13% 524M/4.15G [00:04<00:56, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  13% 545M/4.15G [00:04<00:47, 76.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  14% 566M/4.15G [00:04<00:38, 93.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  14% 587M/4.15G [00:04<00:32, 111MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  15% 619M/4.15G [00:05<00:25, 139MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  15% 640M/4.15G [00:05<00:23, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  16% 661M/4.15G [00:05<00:22, 154MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  16% 682M/4.15G [00:05<00:21, 164MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  17% 703M/4.15G [00:05<00:21, 158MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  17% 724M/4.15G [00:05<00:21, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  18% 755M/4.15G [00:05<00:18, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  19% 786M/4.15G [00:05<00:17, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  19% 807M/4.15G [00:06<00:16, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  20% 828M/4.15G [00:06<00:16, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  20% 849M/4.15G [00:06<00:16, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  21% 870M/4.15G [00:06<00:16, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  21% 891M/4.15G [00:06<00:16, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  22% 912M/4.15G [00:06<00:16, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  22% 933M/4.15G [00:06<00:16, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  23% 954M/4.15G [00:06<00:15, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  23% 975M/4.15G [00:06<00:15, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  24% 996M/4.15G [00:07<00:15, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  25% 1.02G/4.15G [00:07<00:15, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  25% 1.04G/4.15G [00:09<02:00, 25.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  26% 1.07G/4.15G [00:09<01:17, 39.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  26% 1.09G/4.15G [00:09<01:01, 49.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  27% 1.11G/4.15G [00:10<00:51, 59.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  27% 1.13G/4.15G [00:10<00:42, 71.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  28% 1.15G/4.15G [00:10<00:37, 80.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  28% 1.17G/4.15G [00:10<00:34, 85.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  29% 1.20G/4.15G [00:10<00:33, 87.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  29% 1.22G/4.15G [00:10<00:31, 92.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  30% 1.24G/4.15G [00:11<00:29, 98.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  30% 1.26G/4.15G [00:11<00:29, 99.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  31% 1.28G/4.15G [00:11<00:26, 107MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  31% 1.30G/4.15G [00:11<00:26, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  32% 1.32G/4.15G [00:11<00:27, 102MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  32% 1.34G/4.15G [00:12<00:24, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  33% 1.36G/4.15G [00:12<00:22, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  34% 1.39G/4.15G [00:12<00:18, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  34% 1.42G/4.15G [00:12<00:17, 160MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  35% 1.44G/4.15G [00:12<00:16, 167MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  35% 1.46G/4.15G [00:12<00:15, 176MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  36% 1.49G/4.15G [00:12<00:13, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  37% 1.52G/4.15G [00:12<00:12, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  37% 1.55G/4.15G [00:13<00:12, 211MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  38% 1.58G/4.15G [00:13<00:12, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  39% 1.61G/4.15G [00:13<00:12, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  39% 1.64G/4.15G [00:13<00:12, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  40% 1.66G/4.15G [00:13<00:12, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  40% 1.68G/4.15G [00:13<00:12, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  41% 1.70G/4.15G [00:13<00:11, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  41% 1.72G/4.15G [00:13<00:11, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  42% 1.74G/4.15G [00:14<00:11, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  42% 1.76G/4.15G [00:14<00:11, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  43% 1.78G/4.15G [00:14<00:11, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  43% 1.80G/4.15G [00:14<00:11, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  44% 1.82G/4.15G [00:14<00:17, 135MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  45% 1.86G/4.15G [00:14<00:14, 160MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  45% 1.88G/4.15G [00:14<00:15, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  46% 1.90G/4.15G [00:15<00:15, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  46% 1.92G/4.15G [00:15<00:14, 153MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  47% 1.94G/4.15G [00:15<00:13, 160MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  47% 1.96G/4.15G [00:15<00:13, 166MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  48% 1.98G/4.15G [00:15<00:12, 167MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  48% 2.00G/4.15G [00:15<00:13, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  49% 2.02G/4.15G [00:15<00:12, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  49% 2.04G/4.15G [00:15<00:12, 163MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  50% 2.07G/4.15G [00:16<00:12, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  50% 2.09G/4.15G [00:16<00:11, 174MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  51% 2.11G/4.15G [00:16<00:12, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  51% 2.13G/4.15G [00:16<00:11, 174MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  52% 2.15G/4.15G [00:16<00:17, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  52% 2.17G/4.15G [00:16<00:16, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  53% 2.20G/4.15G [00:17<00:12, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  54% 2.23G/4.15G [00:17<00:11, 171MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  54% 2.25G/4.15G [00:17<00:10, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  55% 2.28G/4.15G [00:17<00:10, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  55% 2.30G/4.15G [00:17<00:09, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  56% 2.32G/4.15G [00:17<00:09, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  56% 2.34G/4.15G [00:17<00:09, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  57% 2.36G/4.15G [00:17<00:08, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  57% 2.38G/4.15G [00:17<00:08, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  58% 2.40G/4.15G [00:17<00:08, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  58% 2.42G/4.15G [00:18<00:08, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  59% 2.44G/4.15G [00:18<00:08, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  59% 2.46G/4.15G [00:18<00:08, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  60% 2.50G/4.15G [00:18<00:08, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  61% 2.52G/4.15G [00:18<00:08, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  61% 2.54G/4.15G [00:18<00:07, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  62% 2.56G/4.15G [00:18<00:07, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  62% 2.58G/4.15G [00:19<00:23, 65.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  63% 2.60G/4.15G [00:19<00:19, 79.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  63% 2.62G/4.15G [00:19<00:16, 91.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  64% 2.64G/4.15G [00:20<00:14, 102MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  64% 2.66G/4.15G [00:20<00:12, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  65% 2.69G/4.15G [00:20<00:09, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  66% 2.73G/4.15G [00:20<00:08, 167MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  66% 2.75G/4.15G [00:20<00:08, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  67% 2.78G/4.15G [00:20<00:07, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  67% 2.80G/4.15G [00:20<00:07, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  68% 2.82G/4.15G [00:20<00:06, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  68% 2.84G/4.15G [00:20<00:06, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  69% 2.86G/4.15G [00:21<00:06, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  69% 2.88G/4.15G [00:21<00:06, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  70% 2.90G/4.15G [00:21<00:06, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  70% 2.93G/4.15G [00:21<00:06, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  71% 2.95G/4.15G [00:21<00:05, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  71% 2.97G/4.15G [00:21<00:05, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  72% 2.99G/4.15G [00:21<00:05, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  73% 3.01G/4.15G [00:21<00:05, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  73% 3.03G/4.15G [00:21<00:05, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  74% 3.05G/4.15G [00:21<00:05, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  74% 3.07G/4.15G [00:24<00:43, 24.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  75% 3.09G/4.15G [00:24<00:31, 33.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  75% 3.11G/4.15G [00:24<00:24, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  76% 3.14G/4.15G [00:24<00:18, 56.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  76% 3.16G/4.15G [00:25<00:13, 71.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  77% 3.18G/4.15G [00:25<00:11, 87.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  77% 3.21G/4.15G [00:25<00:08, 116MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  78% 3.23G/4.15G [00:25<00:07, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  78% 3.25G/4.15G [00:25<00:06, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  79% 3.27G/4.15G [00:25<00:05, 159MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  79% 3.29G/4.15G [00:25<00:05, 166MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  80% 3.32G/4.15G [00:25<00:04, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  81% 3.34G/4.15G [00:25<00:04, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  81% 3.37G/4.15G [00:26<00:04, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  82% 3.39G/4.15G [00:26<00:03, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  82% 3.41G/4.15G [00:26<00:03, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  83% 3.43G/4.15G [00:26<00:03, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  83% 3.45G/4.15G [00:26<00:03, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  84% 3.47G/4.15G [00:26<00:03, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  84% 3.49G/4.15G [00:26<00:03, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  85% 3.51G/4.15G [00:26<00:03, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  85% 3.53G/4.15G [00:26<00:03, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  86% 3.55G/4.15G [00:27<00:02, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  86% 3.58G/4.15G [00:27<00:02, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  87% 3.60G/4.15G [00:27<00:02, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  87% 3.62G/4.15G [00:27<00:02, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  88% 3.64G/4.15G [00:27<00:02, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  88% 3.66G/4.15G [00:27<00:02, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  89% 3.68G/4.15G [00:27<00:02, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  89% 3.70G/4.15G [00:27<00:02, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  90% 3.72G/4.15G [00:27<00:02, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  90% 3.74G/4.15G [00:27<00:02, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  91% 3.76G/4.15G [00:28<00:01, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  91% 3.80G/4.15G [00:28<00:01, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  92% 3.82G/4.15G [00:28<00:01, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  92% 3.84G/4.15G [00:28<00:01, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  93% 3.86G/4.15G [00:30<00:08, 32.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  93% 3.88G/4.15G [00:31<00:08, 31.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  94% 3.91G/4.15G [00:31<00:05, 45.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  95% 3.93G/4.15G [00:31<00:04, 54.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  95% 3.95G/4.15G [00:31<00:02, 66.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  96% 3.98G/4.15G [00:31<00:01, 88.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  97% 4.02G/4.15G [00:31<00:01, 114MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  97% 4.04G/4.15G [00:32<00:00, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  98% 4.06G/4.15G [00:32<00:00, 138MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  99% 4.09G/4.15G [00:32<00:00, 159MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  99% 4.12G/4.15G [00:32<00:00, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors: 100% 4.15G/4.15G [00:32<00:00, 128MB/s]\n",
            "Storing https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-AWQ/resolve/b2f7c152209c12057c3a0d77b2c01a1def7d594f/model.safetensors in local_dir at ./Mistral-7B-Instruct-v0.1-AWQ/model.safetensors (not cached).\n",
            "Fetching 10 files: 100% 10/10 [00:33<00:00,  3.39s/it]\n",
            "/content/Mistral-7B-Instruct-v0.1-AWQ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To check gpu\n",
        "import torch\n",
        "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-mIN9uF-ZhK",
        "outputId": "e79918f7-c95e-4be1-8987-f3bd000e2068"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs available: 1\n",
            "GPU 0: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "n_threads = os.cpu_count()\n",
        "n_threads"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AWcavjsm768",
        "outputId": "2576763d-1618-4250-e621-b246f69b0287"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "model_name_or_path = \"./Mistral-7B-Instruct-v0.1-AWQ\"\n",
        "\n",
        "prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The president of the United States is\",\n",
        "    \"The capital of France is\",\n",
        "    \"The future of AI is\",\n",
        "]\n",
        "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
        "\n",
        "llm = LLM(\n",
        "    model=model_name_or_path,\n",
        "    quantization=\"awq\",\n",
        "    dtype='half',\n",
        "    max_model_len=10696 # https://github.com/vllm-project/vllm/issues/1236\n",
        "    )\n",
        "\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlMpaQ3G-tBL",
        "outputId": "53f2001c-beb6-4297-94ea-669d74f579bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 12-01 11:40:55 config.py:398] Casting torch.bfloat16 to torch.float16.\n",
            "WARNING 12-01 11:40:55 config.py:140] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "INFO 12-01 11:40:55 llm_engine.py:72] Initializing an LLM engine with config: model='./Mistral-7B-Instruct-v0.1-AWQ', tokenizer='./Mistral-7B-Instruct-v0.1-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10696, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=awq, seed=0)\n",
            "INFO 12-01 11:41:28 llm_engine.py:207] # GPU blocks: 841, # CPU blocks: 2048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  4.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Hello, my name is', Generated text: \" Marilyn and I'm a human, for those of you who may\"\n",
            "Prompt: 'The president of the United States is', Generated text: ' the chief executive and commander in chief of the Armed Forces of the United States'\n",
            "Prompt: 'The capital of France is', Generated text: ' Paris, a city of art, culture, fashion, and gastronomy.'\n",
            "Prompt: 'The future of AI is', Generated text: ' bright, but it’s important to remember that with every technological advancement comes'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# More about data: https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions\n",
        "\n",
        "# Specify the file path (this path should point to where your file is stored)\n",
        "BASE_PATH = './'\n",
        "file_path = BASE_PATH + 'mtsamples.csv'\n",
        "\n",
        "# Read data as pandas DataFrame\n",
        "data = pd.read_csv(file_path, index_col=0)"
      ],
      "metadata": {
        "id": "jtrMFTuT_3Pg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "TU8KVTIzcpXa",
        "outputId": "556858d0-549b-4643-c3f9-bb2bbc6f3bda"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         description  \\\n",
              "0   A 23-year-old white female presents with comp...   \n",
              "1           Consult for laparoscopic gastric bypass.   \n",
              "2           Consult for laparoscopic gastric bypass.   \n",
              "3                             2-D M-Mode. Doppler.     \n",
              "4                                 2-D Echocardiogram   \n",
              "\n",
              "             medical_specialty                                sample_name  \\\n",
              "0         Allergy / Immunology                         Allergic Rhinitis    \n",
              "1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
              "2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
              "3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
              "4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
              "\n",
              "                                       transcription  \\\n",
              "0  SUBJECTIVE:,  This 23-year-old white female pr...   \n",
              "1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
              "2  HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n",
              "3  2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
              "4  1.  The left ventricular cavity size and wall ...   \n",
              "\n",
              "                                            keywords  \n",
              "0  allergy / immunology, allergic rhinitis, aller...  \n",
              "1  bariatrics, laparoscopic gastric bypass, weigh...  \n",
              "2  bariatrics, laparoscopic gastric bypass, heart...  \n",
              "3  cardiovascular / pulmonary, 2-d m-mode, dopple...  \n",
              "4  cardiovascular / pulmonary, 2-d, doppler, echo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e648a52f-2f8a-40f2-b561-a37f54fa834f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>medical_specialty</th>\n",
              "      <th>sample_name</th>\n",
              "      <th>transcription</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A 23-year-old white female presents with comp...</td>\n",
              "      <td>Allergy / Immunology</td>\n",
              "      <td>Allergic Rhinitis</td>\n",
              "      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n",
              "      <td>allergy / immunology, allergic rhinitis, aller...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Consult for laparoscopic gastric bypass.</td>\n",
              "      <td>Bariatrics</td>\n",
              "      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n",
              "      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n",
              "      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Consult for laparoscopic gastric bypass.</td>\n",
              "      <td>Bariatrics</td>\n",
              "      <td>Laparoscopic Gastric Bypass Consult - 1</td>\n",
              "      <td>HISTORY OF PRESENT ILLNESS: , I have seen ABC ...</td>\n",
              "      <td>bariatrics, laparoscopic gastric bypass, heart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2-D M-Mode. Doppler.</td>\n",
              "      <td>Cardiovascular / Pulmonary</td>\n",
              "      <td>2-D Echocardiogram - 1</td>\n",
              "      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n",
              "      <td>cardiovascular / pulmonary, 2-d m-mode, dopple...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2-D Echocardiogram</td>\n",
              "      <td>Cardiovascular / Pulmonary</td>\n",
              "      <td>2-D Echocardiogram - 2</td>\n",
              "      <td>1.  The left ventricular cavity size and wall ...</td>\n",
              "      <td>cardiovascular / pulmonary, 2-d, doppler, echo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e648a52f-2f8a-40f2-b561-a37f54fa834f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e648a52f-2f8a-40f2-b561-a37f54fa834f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e648a52f-2f8a-40f2-b561-a37f54fa834f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0797a3e6-41f9-4498-9d07-45dc43ed0524\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0797a3e6-41f9-4498-9d07-45dc43ed0524')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0797a3e6-41f9-4498-9d07-45dc43ed0524 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case: Medical specialty classification"
      ],
      "metadata": {
        "id": "BrowZCeoN-Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['medical_specialty'].value_counts(normalize=True).head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCetCvuOdxpi",
        "outputId": "230a155e-72b7-4991-9264-9904acac07c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " Surgery                       0.220644\n",
              " Consult - History and Phy.    0.103221\n",
              " Cardiovascular / Pulmonary    0.074415\n",
              " Orthopedic                    0.071014\n",
              " Radiology                     0.054611\n",
              "Name: medical_specialty, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "specialties = data['medical_specialty'].unique()"
      ],
      "metadata": {
        "id": "aH3C5IflcpaE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "specialties_text = '\\n - '+'\\n - '.join(specialties)\n",
        "print(specialties_text[:200],'\\n...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rkOK6MUfPBT",
        "outputId": "36638954-9d83-45d9-c69b-9dddf963bf41"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " -  Allergy / Immunology\n",
            " -  Bariatrics\n",
            " -  Cardiovascular / Pulmonary\n",
            " -  Neurology\n",
            " -  Dentistry\n",
            " -  Urology\n",
            " -  General Medicine\n",
            " -  Surgery\n",
            " -  Speech - Language\n",
            " -  SOAP / Chart / Progress Notes \n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt\n",
        "prompt_template = \"\"\"\n",
        "You are a nurse reading treatment notes and trying to figure out what topic is discussed and will return the result in JSON output:\n",
        "\n",
        "You are looking for below topics:\"\"\"\n",
        "prompt_template += specialties_text\n",
        "\n",
        "prompt_template += \"\"\"\n",
        "\n",
        "Requirements:\n",
        "• Read the notes carefully and see if the note is talking about the topics you know.\n",
        "• If the note is not talking about the topics you know, figure out it shows other topics.\n",
        "• Present your analysis in a JSON format. The format should be {{\"topic\": \"identified topic or new topic\"}}.\n",
        "\n",
        "Treatment notes to analyze:\n",
        "{text}\n",
        "\"\"\"\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXFe6MQ3fBre",
        "outputId": "f587b900-2821-41a5-ab12-8951789c5f01"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a nurse reading treatment notes and trying to figure out what topic is discussed and will return the result in JSON output:\n",
            "\n",
            "You are looking for below topics:\n",
            " -  Allergy / Immunology\n",
            " -  Bariatrics\n",
            " -  Cardiovascular / Pulmonary\n",
            " -  Neurology\n",
            " -  Dentistry\n",
            " -  Urology\n",
            " -  General Medicine\n",
            " -  Surgery\n",
            " -  Speech - Language\n",
            " -  SOAP / Chart / Progress Notes\n",
            " -  Sleep Medicine\n",
            " -  Rheumatology\n",
            " -  Radiology\n",
            " -  Psychiatry / Psychology\n",
            " -  Podiatry\n",
            " -  Physical Medicine - Rehab\n",
            " -  Pediatrics - Neonatal\n",
            " -  Pain Management\n",
            " -  Orthopedic\n",
            " -  Ophthalmology\n",
            " -  Office Notes\n",
            " -  Obstetrics / Gynecology\n",
            " -  Neurosurgery\n",
            " -  Nephrology\n",
            " -  Letters\n",
            " -  Lab Medicine - Pathology\n",
            " -  IME-QME-Work Comp etc.\n",
            " -  Hospice - Palliative Care\n",
            " -  Hematology - Oncology\n",
            " -  Gastroenterology\n",
            " -  ENT - Otolaryngology\n",
            " -  Endocrinology\n",
            " -  Emergency Room Reports\n",
            " -  Discharge Summary\n",
            " -  Diets and Nutritions\n",
            " -  Dermatology\n",
            " -  Cosmetic / Plastic Surgery\n",
            " -  Consult - History and Phy.\n",
            " -  Chiropractic\n",
            " -  Autopsy\n",
            "\n",
            "Requirements:\n",
            "• Read the notes carefully and see if the note is talking about the topics you know.\n",
            "• If the note is not talking about the topics you know, figure out it shows other topics.\n",
            "• Present your analysis in a JSON format. The format should be {{\"topic\": \"identified topic or new topic\"}}.\n",
            "\n",
            "Treatment notes to analyze:\n",
            "{text}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patient_notes = data['transcription'].loc[0]\n",
        "patient_notes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ouCTV0MxeIJq",
        "outputId": "2a11d03a-c28f-4550-f3ab-3605d5aca30a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f'''<|prompter|>:{prompt_template.format(text=patient_notes)}\\n<|assistant|>:JSON:\\n'''\n",
        "prompts = [prompt]\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.01,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    max_tokens=256\n",
        ")\n",
        "\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "response = outputs[0].outputs[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC2eYglGMxqj",
        "outputId": "d3a73d35-8f6b-44f2-a76a-07922cebf762"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.12s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niXaxFFpNmUR",
        "outputId": "ec0d544e-f139-4b42-8a3b-ef9fe3ad8680"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "\"topic\": \"Allergies\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gC1StcLNgaQ",
        "outputId": "431de295-fb7d-4653-c61a-732ce34335e8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'Allergies'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['medical_specialty'].loc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P_drrB7HOFlN",
        "outputId": "3457c3ca-0e5e-4bf8-85f9-9dd87bd3e87e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Allergy / Immunology'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case: Medical specialty classification & keywords"
      ],
      "metadata": {
        "id": "mMxpHlxyOB-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt\n",
        "prompt_template = \"\"\"\n",
        "You are a nurse reading treatment notes and trying to figure out what topic is discussed and will return the result in JSON output:\n",
        "\n",
        "You are looking for below topics:\"\"\"\n",
        "prompt_template += specialties_text\n",
        "\n",
        "prompt_template += \"\"\"\n",
        "\n",
        "Requirements:\n",
        "• Read the notes carefully and see if the note is talking about the topics you know.\n",
        "• If the note is not talking about the topics you know, figure out it shows other topics.\n",
        "• Extract keywords from the notes that are general and indicative of the identified topic.\n",
        "• Make sure that these keywords are written at treatment notes, don't include numeric numbers.\n",
        "• Please, fill the confidence level about the chosen topic within the next options: very confident, need more info, unconfident, not enough data.\n",
        "• Present your analysis in a JSON format. The format should be {{\"topic\": \"identified topic or new topic\", \"keywords\": [\"keyword1\", \"keyword2\", ...], \"confidence level\": \"very confident|need more info|unconfident|not enough data\", \"explanation\": \"briefly describe your analysis\"}}.\n",
        "\n",
        "\n",
        "Treatment notes to analyze:\n",
        "{text}\n",
        "\"\"\"\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4SJ4z3ZeIPf",
        "outputId": "1496dd42-664c-4a86-f452-f0de9cb84413"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a nurse reading treatment notes and trying to figure out what topic is discussed and will return the result in JSON output:\n",
            "\n",
            "You are looking for below topics:\n",
            " -  Allergy / Immunology\n",
            " -  Bariatrics\n",
            " -  Cardiovascular / Pulmonary\n",
            " -  Neurology\n",
            " -  Dentistry\n",
            " -  Urology\n",
            " -  General Medicine\n",
            " -  Surgery\n",
            " -  Speech - Language\n",
            " -  SOAP / Chart / Progress Notes\n",
            " -  Sleep Medicine\n",
            " -  Rheumatology\n",
            " -  Radiology\n",
            " -  Psychiatry / Psychology\n",
            " -  Podiatry\n",
            " -  Physical Medicine - Rehab\n",
            " -  Pediatrics - Neonatal\n",
            " -  Pain Management\n",
            " -  Orthopedic\n",
            " -  Ophthalmology\n",
            " -  Office Notes\n",
            " -  Obstetrics / Gynecology\n",
            " -  Neurosurgery\n",
            " -  Nephrology\n",
            " -  Letters\n",
            " -  Lab Medicine - Pathology\n",
            " -  IME-QME-Work Comp etc.\n",
            " -  Hospice - Palliative Care\n",
            " -  Hematology - Oncology\n",
            " -  Gastroenterology\n",
            " -  ENT - Otolaryngology\n",
            " -  Endocrinology\n",
            " -  Emergency Room Reports\n",
            " -  Discharge Summary\n",
            " -  Diets and Nutritions\n",
            " -  Dermatology\n",
            " -  Cosmetic / Plastic Surgery\n",
            " -  Consult - History and Phy.\n",
            " -  Chiropractic\n",
            " -  Autopsy\n",
            "\n",
            "Requirements:\n",
            "• Read the notes carefully and see if the note is talking about the topics you know.\n",
            "• If the note is not talking about the topics you know, figure out it shows other topics.\n",
            "• Extract keywords from the notes that are general and indicative of the identified topic.\n",
            "• Make sure that these keywords are written at treatment notes, don't include numeric numbers.\n",
            "• Please, fill the confidence level about the chosen topic within the next options: very confident, need more info, unconfident, not enough data.\n",
            "• Present your analysis in a JSON format. The format should be {{\"topic\": \"identified topic or new topic\", \"keywords\": [\"keyword1\", \"keyword2\", ...], \"confidence level\": \"very confident|need more info|unconfident|not enough data\", \"explanation\": \"briefly describe your analysis\"}}.\n",
            "\n",
            "\n",
            "Treatment notes to analyze:\n",
            "{text}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = f'''<|prompter|>:{prompt_template.format(text=patient_notes)}\\n<|assistant|>:JSON:\\n'''\n",
        "prompts = [prompt]\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.01,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    max_tokens=256\n",
        ")\n",
        "\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "response = outputs[0].outputs[0].text\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "pARftUTLN3dr",
        "outputId": "b424a668-3d3b-4bac-93c7-d134a6d7b75b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.97 s, sys: 16.6 ms, total: 9.99 s\n",
            "Wall time: 11.6 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n\"topic\": \"Allergy / Immunology\",\\n\"keywords\": [\"allergies\", \"rhinitis\", \"Zyrtec\", \"Allegra\", \"nasal sprays\", \"Nasonex\"],\\n\"confidence level\": \"very confident\",\\n\"explanation\": \"The patient presents with a complaint of allergies and has tried several medications for allergic rhinitis. The patient is currently taking Allegra and Ortho Tri-Cyclen. The patient has no known medicine allergies. The patient\\'s vital signs are normal and the physical examination is normal except for mild erythematous nasal mucosa. The assessment is that the patient has allergic rhinitis and the plan is to try Zyrtec again or use loratadine. The patient will also be given samples of Nasonex nasal sprays for three weeks.\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIK_giteN3gY",
        "outputId": "90235201-26ee-4f61-9aef-ab2f11da4ce2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'Allergy / Immunology',\n",
              " 'keywords': ['allergies',\n",
              "  'rhinitis',\n",
              "  'Zyrtec',\n",
              "  'Allegra',\n",
              "  'nasal sprays',\n",
              "  'Nasonex'],\n",
              " 'confidence level': 'very confident',\n",
              " 'explanation': \"The patient presents with a complaint of allergies and has tried several medications for allergic rhinitis. The patient is currently taking Allegra and Ortho Tri-Cyclen. The patient has no known medicine allergies. The patient's vital signs are normal and the physical examination is normal except for mild erythematous nasal mucosa. The assessment is that the patient has allergic rhinitis and the plan is to try Zyrtec again or use loratadine. The patient will also be given samples of Nasonex nasal sprays for three weeks.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[0][['description', 'medical_specialty', 'keywords']].to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkDlTLHFN3ja",
        "outputId": "2868717c-a581-4575-c149-6fddd7c497ba"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': ' A 23-year-old white female presents with complaint of allergies.',\n",
              " 'medical_specialty': ' Allergy / Immunology',\n",
              " 'keywords': 'allergy / immunology, allergic rhinitis, allergies, asthma, nasal sprays, rhinitis, nasal, erythematous, allegra, sprays, allergic,'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M5IQu93sN3lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MnPrsPEQN3oP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}